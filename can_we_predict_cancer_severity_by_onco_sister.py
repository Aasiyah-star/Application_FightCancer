# -*- coding: utf-8 -*-
"""can_we_predict_cancer_severity-by-onco_sister.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/gist/Aasiyah-star/1b5a3aff67f06748781499f5b7e1bc7d/can_we_predict_cancer_severity-by-onco_sister.ipynb

# üéØ Predicting Cancer Severity from Patient Data

**Introduction : Peut-on pr√©dire le risque de cancer √† partir des habitudes de vie ?**


Le cancer est l'une des principales causes de mortalit√© dans le monde. Si des facteurs g√©n√©tiques jouent un r√¥le, de nombreux cancers sont influenc√©s par des **facteurs comportementaux et environnementaux**, tels que l‚Äôalimentation, le tabagisme, le stress ou encore l‚Äôacc√®s aux soins.

Ce projet s‚Äôinscrit dans une d√©marche de **pr√©vention**, en explorant la possibilit√© d‚Äôanticiper le risque de cancer √† partir des **habitudes de vie d√©clar√©es par les patients**.
Nous nous appuyons sur un jeu de donn√©es riche et vari√©, qui regroupe des informations de sant√©, de mode de vie, de statut socio-√©conomique et de conditions m√©dicales.

Nous allons ici :

*   Nettoyer et pr√©parer les donn√©es,
*   S√©lectionner les variables pertinentes,
*   Construire diff√©rents **mod√®les pr√©dictifs** (Random Forest, R√©gression Logistique),
*   Am√©liorer l‚Äô√©quilibre du jeu de donn√©es avec **SMOTE**,
*   Et √©valuer les performances via une **validation crois√©e**.


L‚Äôobjectif final est de **comprendre quels facteurs sont les plus pr√©dictifs du cancer** et d‚Äô√©valuer si une mod√©lisation fiable est possible √† partir de donn√©es d√©claratives.

## I. Data Discovery

#####This section aims to better understand the structure, content, and quality of the data used to predict cancer severity. It includes an overview of the columns, data types, missing values, and descriptive statistics.
"""

import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px

# Monter Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Lire le fichier SAS depuis Google Drive
import pandas as pd
file_path = '/content/drive/MyDrive/hints7_public.sas7bdat'

df_original = pd.read_sas(file_path, format='sas7bdat')
df=df_original.copy()

# Dataset Overview

df.head()

df['BirthSex']

# Dimensions and structure

df.shape  # Nombre de lignes et de colonnes
df.info()  # Types de variables et non-null

# Descriptive statistics

df.describe()

# Missing Data

df.isnull().sum().sort_values(ascending=False)

# Types de variables
print(df.dtypes)

pd.set_option('display.max_columns', None)

# Replace no response, no answer [-9 : -1] values with '-1'

no_response = [ -9, #Missing data (Not Ascertained)
                -7, #Missing data (Web partial - Question Never Seen)
                -6, #Missing data (Filter Missing)
                -5, #Multiple responses selected in error
                -4, # Unreadable or Non-conforming numeric response
                -2, #Question answered in error (Commission Error)
                -1 #Inapplicable
               ]

# 0 : None / Never / Zero / No

#Concate num & object & Date (update) columns
col_numeric = df.select_dtypes(include='number').columns
col_object = df.select_dtypes(exclude='number').columns
df_numeric = df[col_numeric].replace(no_response, -1)
df_object = df_original[col_object].copy()
df = pd.concat([df_numeric, df_object], axis=1)

"""## II. Dictionary of questions

Ce dictionnaire regroupe l‚Äôensemble des variables issues du questionnaire initial, accompagn√©es de leurs descriptions.Il nous permet de mieux comprendre la signification de chaque colonne et de guider notre nettoyage des donn√©es.C‚Äôest une √©tape essentielle pour assurer la coh√©rence et la lisibilit√© de notre analyse.

 *Les lettres de C √† R correspoonde au √† la num√©rotation des questions du questionnaires.*

###  C. Analyse des soins de sant√© (consultations, qualit√© per√ßue, confiance) ü©∫
"""

df_health_Care = df[['FreqGoProvider', 'QualityCare', 'TrustHCSystem']]

"""###  G. Tests g√©n√©tiques : variables li√©es aux tests pass√©s et aux motivations üß¨

Ce sous-ensemble contient toutes les variables relatives au fait d‚Äôavoir pass√© un test g√©n√©tique, ainsi que les raisons d√©clar√©es.

"""

df_Genetic_Testing = df[['HadTest3_Ancestry2','HadTest3_PersonalTrait','HadTest3_SpecificDisease','HadTest3_Prenatal',
                         'HadTest3_Other','HadTest3_NotSure','HadTest3_NotHad','HadTest3_Cat','ReasonTest_DocRec','ReasonTest_UnderstandFam',
                         'ReasonTest_FindFam','ReasonTest_PersTraits', 'ReasonTest_DiseaseRisk','ReasonTest_LearnStrategies','ReasonTest_Prenatal',
                         'ReasonTest_Gift','ReasonTest_Other','ReasonTest_Other_OS','ReasonTest_Cat']]
df_Genetic_Testing

"""##### üõ†Ô∏è Nettoyage et transformation des colonnes
Dans cette section, nous harmonisons les noms de colonnes et transformons certaines valeurs pour faciliter l‚Äôanalyse.

"""

def replace_values(df, columns, old_values, new_values):
    for col in columns:
        df.loc[:, col] = df[col].replace(old_values, new_values)

cols_to_replace5 = ['HadTest3_Ancestry2', 'HadTest3_PersonalTrait', 'HadTest3_SpecificDisease', 'HadTest3_Prenatal',
                    'HadTest3_Other', 'HadTest3_NotSure']
replace_values(df_Genetic_Testing, cols_to_replace5, 2, 0)

def replace_values(df, cols, hints_code, our_code):
    for col in cols:
        if isinstance(hints_code, list) and isinstance(our_code, list):
            df[col] = df[col].replace(hints_code, our_code)
        else:
            df[col] = df[col].replace(hints_code, our_code)

replace_values(df_Genetic_Testing, ['ReasonTest_Cat'], [2, 3, 4, 5, 6, 7, 8, 9, 91], 1)

# 1.Replace Never/no from 2 to 0
cols_to_replace5 = ['HadTest3_Ancestry2', 'HadTest3_PersonalTrait', 'HadTest3_SpecificDisease', 'HadTest3_Prenatal',
                    'HadTest3_Other', 'HadTest3_NotSure']

for col in cols_to_replace5:
  df_Genetic_Testing.loc[:, col] = df_Genetic_Testing[col].replace(2, 0)

#df_Genetic_Testing[['ReasonTest_Cat'] : [2, 3, 4, 5, 6, 7, 8, 9, 91]]
other_response = [2, 3, 4, 5, 6, 7, 8, 9, 91]

True_response = [1] # Doctor's recommendation only

# Apply the replacement for 'other_response' to the relevant column(s)
# Assuming you want to apply this to 'ReasonTest_Cat' based on the legend provided earlier
df_Genetic_Testing['ReasonTest_Cat'] = df_Genetic_Testing['ReasonTest_Cat'].replace(other_response, 2)

# Apply the replacement for 'True_response' to the relevant column(s)
df_Genetic_Testing['ReasonTest_Cat'] = df_Genetic_Testing['ReasonTest_Cat'].replace(True_response, 1)

# Now, apply value_counts to the specific column you want to analyze
df_Genetic_Testing['ReasonTest_Cat'].value_counts().sort_values(ascending=False)

other_response_hadtest = [  8, 10, 15, 16, 18, 19, 91 ]
true_response_hadtest = [ 17 ]
df_Genetic_Testing['HadTest3_Cat'] = df_Genetic_Testing['HadTest3_Cat'].replace(other_response_hadtest, 2)
df_Genetic_Testing['HadTest3_Cat'] = df_Genetic_Testing['HadTest3_Cat'].replace(true_response_hadtest, 1)
df_Genetic_Testing['HadTest3_Cat'].value_counts().sort_values(ascending=False)

"""### I. Analyse de la perception de la sant√© globale

Dans cette section, nous explorons la variable li√©e √† la perception de l‚Äô√©tat de sant√© g√©n√©ral des participants √† l‚Äôenqu√™te.
Cela peut fournir des indications pr√©cieuses sur le bien-√™tre ressenti et sa corr√©lation avec d'autres facteurs (mode de vie, acc√®s aux soins, etc.).

"""

df_overall_health = df[['GeneralHealth', 'OwnAbilityTakeCareHealth','HealthLimits_Deaf','HealthLimits_Blind','HealthLimits_Mobility','HealthLimits_Pain','MedConditions_Diabetes', 'MedConditions_HighBP', 'MedConditions_HeartCondition', 'MedConditions_LungDisease', 'MedConditions_Depression','Height_Feet', 'Height_Inches','Weight','SleepWeekdayHr','SleepWeekendHr2','LittleInterest','Hopeless','Nervous','Worrying','FeelLeftOut','FeelPeopleBarelyKnow', 'FeelIsolated', 'FeelPeopleNotWithMe','TimesModerateExercise','HowLongModerateExerciseMinutes','TimesStrengthTraining','AverageTimeSitting']].copy()
df_overall_health

"""#### üõ†Ô∏è Nettoyage et transformation des colonnes
Dans cette section, nous harmonisons les noms de colonnes et transformons certaines valeurs pour faciliter l‚Äôanalyse.

##### Standardisation des r√©ponses n√©gatives (Remplacement par 0)
"""

# Replace 2 to 0 for the "No" for all these columns :
Health_limits_col = [
    'HealthLimits_Deaf', 'HealthLimits_Blind', 'HealthLimits_Mobility', 'HealthLimits_Pain']
Med_conditions_col = [
    'MedConditions_Diabetes', 'MedConditions_HighBP', 'MedConditions_HeartCondition',
    'MedConditions_LungDisease', 'MedConditions_Depression']
cols_to_replace = Health_limits_col + Med_conditions_col
df_overall_health[cols_to_replace] = df_overall_health[cols_to_replace].replace(2, 0)

# Replace Never/no from 4 to 0
cols_to_replace2 = ['LittleInterest', 'Hopeless', 'Nervous', 'Worrying']
for col in cols_to_replace2:
    df_overall_health[col] = df_overall_health[col].replace(4, 0)

"""#####  √âtude de l'IMC (Body Mass Index) des participants"""

# Convert heigh_feet and height_inches to height_m
df_overall_health.loc[:, 'Height_m'] = (df_overall_health['Height_Feet'] * 30.48 + df_overall_health['Height_Inches'] * 2.54) / 100

# Convert weight to kg
df_overall_health.loc[:, 'Weight_kg'] = df_overall_health['Weight'] * 0.453592

# Calculate BMI given weight_kg and height_meters
def calculate_bmi(weight_kg, height_m):
    if height_m <= 0:
        bmi= -1   # Avoid division by zero or negative height
    else :
      bmi = weight_kg / (height_m ** 2)

    if bmi < 18.5:
        interpretation = "Underweight"
    elif 18.5 <= bmi < 25:
        interpretation = "Normal weight"
    elif 25 <= bmi < 30:
        interpretation = "Overweight"
    else:
        interpretation = "Obesity"
    return bmi, interpretation

# Apply BMI and BMI_interpretation
df_overall_health[['BMI', 'BMI_Category']] = df_overall_health.apply(lambda row: pd.Series(calculate_bmi(row['Weight_kg'], row['Height_m'])), axis=1 )

"""#####  √âtude des niveaux d‚Äôexercice mod√©r√© chez les participants"""

df_overall_health['Moderate_exercise_min_week'] = df_overall_health['TimesModerateExercise'] * df_overall_health['HowLongModerateExerciseMinutes']

"""##### Cr√©ation d'une nouvelle colonne"""

# Create a new column 'Health_limits' that sums Health_limits_col for positive values only, not -1 and returns -1 if all values are negative
def conditional_sum(row):
    value = row[Health_limits_col]
    if (value >= 0).any():
        return value[value > 0].sum()
    else:
        return -1

# Apply the function to each row in df_overall_health # verif : ok a priori
df_overall_health['Health_limits'] = df_overall_health.apply(conditional_sum, axis=1)

# Create a new column 'Bad_Med_cond' that sums Med_conditions_col for positive values only, not -1 and returns -1 if all values are negative

def conditional_sum(row):
    value = row[Med_conditions_col]
    if (value >= 0).any():
        return value[value > 0].sum()
    else:
        return -1

# Apply the function to each row in df_overall_health # verif : ok a priori
df_overall_health['Bad_Med_cond'] = df_overall_health.apply(conditional_sum, axis=1)

# Create a new column ['SleepWeek'] = Sleeping time per week (from monday to Sunday)
df_overall_health.loc[:, 'SleepWeek'] = df_overall_health['SleepWeekdayHr'] + df_overall_health['SleepWeekendHr2']

# drop height_feet and _inches because already cnverted in meters
# drop Weight in LBS because aleady converted in kg
df_overall_health.drop(columns=['Height_Feet', 'Height_Inches', 'Weight'], inplace=True)

# Replace DataFrame original values with our own code
cols_to_replace2 = ['FeelLeftOut' , 'FeelPeopleBarelyKnow', 'FeelIsolated', 'FeelPeopleNotWithMe']
hints_list = [-1, 1, 2, 3 ,4, 5]
our_code = [-1, 0, 1, 2, 3, 4]
for col in cols_to_replace2:
    df_overall_health[col] = df_overall_health[col].replace(hints_list, our_code)

# Replace DataFrame original values with our own code
cols_to_replace = ['Nervous' , 'Worrying', 'Hopeless','LittleInterest']
hints_list = [-1, 4, 3, 2 ,1]
our_code = [-1, 0, 1, 2, 3]
for col in cols_to_replace:
    df_overall_health[col] = df_overall_health[col].replace(hints_list, our_code)

columns_overall_health = df_overall_health.columns.tolist()
columns_overall_health

"""###  J. Exploration des donn√©es environnementales et indicateurs de sant√© üåø

Ce segment explore les perceptions et les effets de l‚Äôenvironnement sur la sant√©. Il inclut des √©l√©ments li√©s √† la qualit√© de l‚Äôair, au changement climatique, ainsi qu‚Äô√† l‚Äôexposition au soleil. Les comportements associ√©s √† ces expositions sont √©galement pris en compte. L‚Äôobjectif est d‚Äôidentifier d‚Äô√©ventuelles corr√©lations entre environnement, habitudes de vie et √©tat de sant√©. Ce sous-ensemble contribue √† une meilleure compr√©hension des facteurs environnementaux sur le bien-√™tre individuel.
"""

df_Environment_Health=df[['ClimateChgHarmHealth','HarmHealth_OutdoorAir','HarmHealth_IndoorAir','TimesSunburned','Sunburned_Alcohol2','Sunburned_Marijuana']]
df_Environment_Health

"""### K. Sant√© & Stress Financier

Cette section s‚Äôint√©resse aux pr√©occupations financi√®res et √† leur influence sur la sant√©.Elle √©value le stress li√© aux d√©penses m√©dicales ou au quotidien.L‚Äôobjectif est de mieux comprendre l‚Äôimpact √©conomique sur le bien-√™tre g√©n√©ral.
"""

df_financial_concern=df[['CutSkipMeals2','CannotAffordMeals2','WorryForcedMove2','LackTransportation2','DiffPayMedBills']]
df_financial_concern

"""### L.Analyse des Comportements Nutritionnels"""

df_Health_Nutrition = df[['Fruit2','Vegetables2']]
df_Health_Nutrition

"""### M. Analyse de la Consomation d'Alcool"""

df_alcool=df[['DrinkDaysPerMonth','DrinksPerDay2','DrinksOneOccasion']]
df_alcool

"""##### üõ†Ô∏è Nettoyage et transformation des colonnes
Dans cette section, nous harmonisons les noms de colonnes et transformons certaines valeurs pour faciliter l‚Äôanalyse.
"""

# Create a new column 'Drink_nb_PerMonth'  for positive values only in DrinkDaysPerMonth AND DrinksPerDay2, not -1 and returns -1 if all values are negative

def NbDrinkPerMonth(row):
    if row['DrinkDaysPerMonth'] > 0 and row['DrinksPerDay2'] > 0:
        return row['DrinkDaysPerMonth'] * row['DrinksPerDay2']
    else:
        return -1

# Apply the function to each row in df_alcool # verif : ok a priori
df_alcool.loc[:, 'Drink_nb_PerMonth'] = df_alcool.apply(NbDrinkPerMonth, axis=1)

"""### N. Analyses du comportements de Consommation : Tabac & Cannabis"""

df_tabacco_product=df[['Smoke100','SmokeNow','EverUsed_ECig','EverUsed_Cigars','EverUsed_Hookah','EverUsed_Smokeless','EverUsed_NicPouch','EverUsed_HeatTob','EverUsed_ModRisk','EverUsed_None','EverUsed_Cat','NowUse_ECig','NowUse_Cigars','NowUse_Hookah','NowUse_Smokeless','NowUse_NicPouch','NowUse_HeatTob','NowUse_ModRisk','NowUse_None','NowUse_Cat','MarijuanaUseReason']]
df_tabacco_product

"""##### üõ†Ô∏è Nettoyage et transformation des colonnes

Dans cette section, nous harmonisons les noms de colonnes et transformons certaines valeurs pour faciliter l‚Äôanalyse.
"""

use_tabacco = [ 1,2,3,4,5,6,7,9]
no_use_tabacco = [ 8 ]
df_tabacco_product['EverUsed_Cat'] = df_tabacco_product['EverUsed_Cat'].replace(use_tabacco, 1)
df_tabacco_product['EverUsed_Cat'] = df_tabacco_product['EverUsed_Cat'].replace(no_use_tabacco, 2)
df_tabacco_product['EverUsed_Cat'].value_counts().sort_values(ascending=False)

# Define the values to be replaced and the new values
use_marijuana = [1,2,3,4,5,6,7,9]  # For medical, recreational, or both
no_use_marijuana = [8]    # Missing data (based on prior no_response definition)

df_tabacco_product['NowUse_Cat'] = df_tabacco_product['NowUse_Cat'].replace(use_marijuana, 1)
df_tabacco_product['NowUse_Cat'] = df_tabacco_product['NowUse_Cat'].replace(no_use_marijuana, 2)

# Now, apply value_counts to the new column
df_tabacco_product['NowUse_Cat'].value_counts().sort_values(ascending=False)

"""### O. Analyse des Pratiques de D√©pistage du Cancer"""

df_diagnosed_cancer=df[['DocTalkLDCT','DocTellColorectalTests2']]
df_diagnosed_cancer

"""##### üõ†Ô∏è Nettoyage et transformation des colonnes

R√©-encodage des r√©ponses pour simplifier et structurer les donn√©es en vue d‚Äôune analyse pr√©dictive plus fiable.

Afin d‚Äôam√©liorer la lisibilit√© et la performance de notre mod√®le pr√©dictif, nous r√©-encodons les r√©ponses en valeurs binaires/coh√©rentes via un dictionnaire de remplacement.
"""

# Recodage manuel pour la colonne 'DocTalkLDCT'
# 2 = Oui ‚Üí 1, 1 et 3 = Autres r√©ponses ‚Üí 0, 4 = Manquant ‚Üí -1

df_diagnosed_cancer['DocTalkLDCT'] = df_diagnosed_cancer['DocTalkLDCT'].replace(2, 1)
df_diagnosed_cancer['DocTalkLDCT'] = df_diagnosed_cancer['DocTalkLDCT'].replace([1, 3], 0)
df_diagnosed_cancer['DocTalkLDCT'] = df_diagnosed_cancer['DocTalkLDCT'].replace(4, -1)

# V√©rification des nouvelles valeurs
df_diagnosed_cancer['DocTalkLDCT'].value_counts(dropna=False)

# Recodage manuel pour la colonne 'DocTalkLDCT'
# 2 = Oui ‚Üí 1, 1 et 3 = Autres r√©ponses ‚Üí 0, 4 = Manquant ‚Üí -1

df_diagnosed_cancer['DocTalkLDCT'] = df_diagnosed_cancer['DocTalkLDCT'].replace(2, 1)
df_diagnosed_cancer['DocTalkLDCT'] = df_diagnosed_cancer['DocTalkLDCT'].replace([1, 3], 0)
df_diagnosed_cancer['DocTalkLDCT'] = df_diagnosed_cancer['DocTalkLDCT'].replace(4, -1)

# V√©rification des nouvelles valeurs
df_diagnosed_cancer['DocTalkLDCT'].value_counts(dropna=False)

print(df_diagnosed_cancer.columns.tolist())

# Recodage manuel pour la colonne 'DocTellColorectalTests2'
# 1 = r√©ponse √† garder ‚Üí 1
# 2 et 3 = autres r√©ponses ‚Üí 0

df_diagnosed_cancer['DocTellColorectalTests2'] = df_diagnosed_cancer['DocTellColorectalTests2'].replace(1, 1)
df_diagnosed_cancer['DocTellColorectalTests2'] = df_diagnosed_cancer['DocTellColorectalTests2'].replace([2, 3], 0)

# V√©rification des r√©sultats
df_diagnosed_cancer['DocTellColorectalTests2'].value_counts(dropna=False)

"""### I.Analyse des Cas de Cancer D√©clar√©s"""

df_diagnosed_cancer=df[['CaBladder','CaBone','CaBreast','CaBrain','CaCervical','CaColon','CaEndometrial','CaEye','CaHeadNeck','CaLeukemia','CaLiver','CaLung','CaHodgkins','CaNonHodgkin','CaMelanoma', 'CaMultMyeloma','CaOral','CaOvarian','CaPancreatic','CaPharyngeal','CaProstate','CaRectal','CaRenal','CaSkin','CaStomach','CaTesticular','CaThyroid','CaOther','CaOther_OS','Cancer_Cat','WhenDiagnosedCancer','FamilyEverHadCancer2', 'EverHadCancer']]
df_diagnosed_cancer

"""##### üõ†Ô∏è Nettoyage et transformation des colonnes

R√©-encodage des r√©ponses pour simplifier et structurer les donn√©es en vue d‚Äôune analyse pr√©dictive plus fiable.

Afin d‚Äôam√©liorer la lisibilit√© et la performance de notre mod√®le pr√©dictif, nous r√©-encodons les r√©ponses en valeurs binaires/coh√©rentes via un dictionnaire de remplacement.
"""

other_response_hadtest = [ 3 ]
df_diagnosed_cancer['FamilyEverHadCancer2'] = df_diagnosed_cancer['FamilyEverHadCancer2'].replace(other_response_hadtest, 2)
df_diagnosed_cancer['FamilyEverHadCancer2'].value_counts().sort_values(ascending=False)

other_response_hadtest = [b'-1', b'-7', b'-6', b'-2']
df_diagnosed_cancer['CaOther_OS'] = df_diagnosed_cancer['CaOther_OS'].replace(other_response_hadtest, -1)
df_diagnosed_cancer['CaOther_OS'].value_counts().sort_values(ascending=False)

othercancer_response = [ b'VAGINAL',
                b'PENILE',
                b'ESOPHAGEAL',
                b'POLYCYTHEMIA',
                b'GIST TUMOR',
                b'SUN SPOT FOREHEAD',
                b'APPENDIX CANCER',
                b'VOCAL CORDS',
                b'HIPPO & PARTIAL LOBECTOMY',
                b'SARCOMA',
                b'MYELODYSPLASTIC SYNDROME',
                b'SQUAMOUS ON MY VULVA',
                b'NEO',
                b'LYMPH NODES',
                b'BLOOD DISORDER',
                b'+2 OTHER RARE CANCER',
                b'NASAL',
                b'URETERAL CANCER',
                b'SANOVIAL SARCOMA',
                b'CANCER IN AXILLA',
                b'T-CELL LYMPHOMA',
                b'FIBROUS HISTIOCYTOMA',
                b'NET',
                b'CLUS',
                b'MOLAR PREGNANCY',
                b'ANAL',
                b'PAROTID GLAND CANCER',
                b'ANGIOSARCOMA',
                b'BLOOD PLATELETS',
                b'LIP',
                b'NEUROEDOCRINE APPY',
                b'MONOCOLONAL B-CELL LYMPHOCYTOSIS (PRE-CANCEROUS FOR CLL)',
                b'NEUROENDOCRINE TUMORS',
                b'-9',
                b'POLYPS',
                b'SERPENZ',
               ]
df_diagnosed_cancer['CaOther_OS'] = df_diagnosed_cancer['CaOther_OS'].replace(othercancer_response, 1)
df_diagnosed_cancer['CaOther_OS'].value_counts().sort_values(ascending=False)

"""### R. Analyses du contexte familiale"""

df_tabacco_product=df[['Smoke100','SmokeNow','EverUsed_ECig','EverUsed_Cigars','EverUsed_Hookah',
                       'EverUsed_Smokeless','EverUsed_NicPouch','EverUsed_HeatTob','EverUsed_ModRisk','EverUsed_None',
                       'EverUsed_Cat','NowUse_ECig','NowUse_Cigars','NowUse_Hookah','NowUse_Smokeless','NowUse_NicPouch','NowUse_HeatTob',
                       'NowUse_ModRisk','NowUse_None','NowUse_Cat','MarijuanaUseReason']]
df_tabacco_product

"""## III. Cr√©ation d'un nouveau DataFrame


Une fois le dictionnaire cr√©√©, nous allons construire un nouveau DataFrame contenant uniquement les donn√©es utiles √† notre mod√®le.
"""

df_Household =df[['Age','BirthSex','WorkHrsPerWeek','Occupation2_Employed','Occupation2_OOW1Y','Occupation2_OOWLess1Y','Occupation2_Homemaker',
                  'Occupation2_Student','Occupation2_Retired','Occupation2_Disabled','Occupation2_Other','Occupation2_Other_OS','Occupation2_Cat',
                  'FullTimeOcc2_Cat','WorkHrs_DidNotWork','WorkHrs_EarlyAM','WorkHrs_AM','WorkHrs_Afternoon','WorkHrs_Evening','WorkHrs_Night','WorkHrs_Cat',
                  'MaritalStatus','Education','NotHisp',
                  'TotalHousehold','ChildrenInHH','IncomeRanges','IncomeFeelings',
                  'Mexican','PuertoRican','Cuban','OthHisp','Hisp_Cat','White','Black','AmerInd','AsInd','Chinese','Filipino',
                  'Japanese','Korean','Vietnamese','OthAsian','OthPacIsl','Race_Cat2']]
df_Household

df_Household = df_Household.copy()

df_Household.info()

"""#### üõ†Ô∏è Nettoyage et transformation des colonnes
Dans cette section, nous harmonisons les noms de colonnes et transformons certaines valeurs pour faciliter l‚Äôanalyse.

##### Standardisation (Remplacement par 0)
"""

def replace_values(df, columns, old_values, new_values):
    for col in columns:
        df.loc[:, col] = df[col].replace(old_values, new_values)

def replace_values(df, cols, old_values, new_values):
    for col in cols:
        df[col] = df[col].replace(old_values, new_values)

# 1. Remplacer les 2 par 0
cols1 = ['Occupation2_Homemaker', 'Occupation2_Student', 'Occupation2_Retired', 'Occupation2_Disabled',
         'WorkHrs_DidNotWork', 'WorkHrs_EarlyAM', 'WorkHrs_AM', 'WorkHrs_Evening', 'NotHisp', 'Mexican',
         'PuertoRican', 'Cuban', 'OthHisp', 'White', 'Black', 'AmerInd', 'AsInd', 'Chinese', 'Filipino',
         'Japanese', 'Korean', 'Vietnamese', 'OthAsian', 'OthPacIsl']
replace_values(df_Household, cols1, 2, 0)

cols2 = ['EverHadCancer','CaBladder','CaBone','CaBreast','CaBrain','CaCervical','CaColon','CaEndometrial','CaEye',
         'CaHeadNeck','CaLeukemia','CaLiver','CaLung','CaHodgkins','CaNonHodgkin','CaMelanoma','CaMultMyeloma',
         'CaOral','CaOvarian','CaPancreatic','CaPharyngeal','CaProstate','CaRectal','CaRenal','CaSkin',
         'CaStomach','CaTesticular','CaThyroid','CaOther','CaOther_OS','Cancer_Cat','WhenDiagnosedCancer','FamilyEverHadCancer2']
replace_values(df_diagnosed_cancer, cols2, 2, 0)

cols3 = ['HadTest3_Ancestry2','HadTest3_PersonalTrait','HadTest3_SpecificDisease','HadTest3_Prenatal',
         'HadTest3_Other','HadTest3_NotSure','HadTest3_NotHad','ReasonTest_DocRec','ReasonTest_UnderstandFam',
         'ReasonTest_FindFam','ReasonTest_PersTraits','ReasonTest_DiseaseRisk','ReasonTest_LearnStrategies',
         'ReasonTest_Prenatal','ReasonTest_Gift','ReasonTest_Other']
replace_values(df_Genetic_Testing, cols3, 2, 0)

cols4 = ['Smoke100','EverUsed_ECig','EverUsed_Cigars','EverUsed_Hookah','EverUsed_Smokeless',
         'EverUsed_NicPouch','EverUsed_HeatTob','EverUsed_ModRisk','EverUsed_None','EverUsed_Cat',
         'NowUse_ECig','NowUse_Cigars','NowUse_Hookah','NowUse_Smokeless','NowUse_NicPouch',
         'NowUse_HeatTob','NowUse_ModRisk','NowUse_None','NowUse_Cat']
replace_values(df_tabacco_product, cols4, 2, 0)

# 2. Autres remplacements avec mapping complexe
replace_values(df_tabacco_product, ['SmokeNow'], [-1, 1, 2, 3], [-1, 2, 1, 0])
replace_values(df_Household, ['BirthSex'], 3, 0)
replace_values(df_health_Care, ['QualityCare'], [-1, 5, 4, 3, 2, 1], [-1, 0, 1, 2, 3, 4])
replace_values(df_health_Care, ['TrustHCSystem'], [-1, 1, 2, 3, 4], [-1, 3, 2, 1, 0])
replace_values(df_overall_health, ['GeneralHealth', 'OwnAbilityTakeCareHealth'], [-1, 5, 4, 3, 2 ,1], [-1, 0, 1, 2, 3, 4])

"""##### Person_w


"""

df_Person_w = (df_original.loc[:, 'PERSON_FINWT0':'PERSON_FINWT50'].copy())
df_Person_w

print(df_diagnosed_cancer)

df2 = pd.concat([df_health_Care, df_Genetic_Testing, df_overall_health,df_Environment_Health, df_financial_concern, df_Health_Nutrition, df_alcool, df_tabacco_product,df_diagnosed_cancer, df_diagnosed_cancer, df_Household, df_Person_w], axis=1)
df2

df2.info()
# 7278 row ; 193 entries

"""### √âvaluation de la qualit√© des donn√©es

###### Avant de poursuivre notre analyse, il est important d‚Äôidentifier les colonnes contenant des valeurs manquantes. Cela nous permettra de d√©cider si certaines variables doivent √™tre nettoy√©es, imput√©es ou supprim√©es.
"""

df2.isna().sum().sort_values(ascending=False)

df_final = df2.replace(-1, pd.NA, inplace=False)

print(df_final.columns.tolist())

df_final.isna().mean().sort_values(ascending=False)

(df_final.isna().mean() * 100).sort_values(ascending=False)

df_final.head()

"""### Analyse des colonnes avec valeurs manquantes

Dans cette √©tape, nous avons calcul√© la proportion de valeurs manquantes (`NaN`) pour chaque colonne du DataFrame final. L'objectif est d‚Äôidentifier les variables dont la qualit√© des donn√©es est insuffisante.  
Nous avons s√©lectionn√© les colonnes dont plus de 50 % des donn√©es sont manquantes et les avons stock√©es dans un DataFrame √† part (`df_filtered`) √† titre informatif.

Cela nous permet de :
- Surveiller les variables potentiellement inutilisables pour la mod√©lisation ;
- D√©cider plus tard si nous les excluons, les imputons, ou les analysons s√©par√©ment.

Pour l‚Äôinstant, ces colonnes ne sont pas supprim√©es du DataFrame principal (`df_final`).
"""

# Calcule la proportion de NaN dans df2
nan_ratio = df_final.isna().mean()
print(nan_ratio)

# S√©lectionne les colonnes avec plus de 50 % de NaN
cols_over_50_nan = nan_ratio[nan_ratio > 0.5].index
print(cols_over_50_nan)

# Garde ces colonnes dans df
df_filtered = df2[cols_over_50_nan]
print(df_filtered)

"""### Tel√©lchargement du fichier en Version CSV"""

df_final.to_csv('df_final.csv', index=False)

"""### Am√©lioration et Modification des cat√©gorielle

##### SleepCategory
"""

df_final['SleepWeekper'] = df_final['SleepWeek'] // 2
df_final['SleepWeekper']

print(df_final['SleepWeekper'].value_counts())

import pandas as pd
import plotly.express as px

# ‚úÖ Nettoyer et convertir la colonne existante
df_final['SleepWeekdayHr'] = pd.to_numeric(df_final['SleepWeekdayHr'], errors='coerce')

# ‚úÖ Classification
def classify_sleep(hours):
    if pd.isna(hours) or hours < 0:
        return 'Inconnu'
    elif hours <= 5:
        return 'Tr√®s court √† court'
    elif hours in [6, 7, 8]:
        return 'Normal'
    elif hours in [9, 10]:
        return 'Long'
    elif hours >= 11:
        return 'Tr√®s long'
    else:
        return 'Inconnu'

df_final['SleepCategory'] = df_final['SleepWeekdayHr'].apply(classify_sleep)
df_final['SleepCategory'] = pd.Categorical(df_final['SleepCategory'],
                                           categories=['Tr√®s court √† court', 'Normal', 'Long', 'Tr√®s long', 'Inconnu'],
                                           ordered=True)

# ‚úÖ Comptage pour le graphique
sleep_counts = df_final['SleepCategory'].value_counts().reindex(df_final['SleepCategory'].cat.categories).reset_index()
sleep_counts.columns = ['SleepCategory', 'count']

# ‚úÖ Graphique
fig = px.bar(
    sleep_counts,
    x='SleepCategory',
    y='count',
    text='count',
    labels={'SleepCategory': 'Cat√©gorie de sommeil', 'count': 'Nombre de personnes'},
    title='Distribution des cat√©gories de sommeil (jours de semaine)'
)

fig.update_traces(textposition='outside')
fig.update_layout(xaxis_title='Cat√©gorie de sommeil', yaxis_title='Nombre de r√©pondants')
fig.show()

df_final['SleepCategory']

"""##### DrinkDaysPerMonth"""

df_final['DrinkDaysPerMonth'].value_counts()

import pandas as pd
import plotly.express as px

# ‚úÖ Fonction de cat√©gorisation
def categorize_drink_days(days):
    if pd.isna(days) or days < 0:
        return 'Inconnu'
    elif days == 0:
        return 'Aucun'
    elif 1 <= days <= 3:
        return 'Rare'
    elif 4 <= days <= 9:
        return 'Occasionnel'
    elif 10 <= days <= 19:
        return 'R√©gulier'
    elif 20 <= days <= 29:
        return 'Fr√©quent'
    elif days == 30:
        return 'Quotidien'
    else:
        return 'Inconnu'

# ‚úÖ Appliquer la fonction
df_final['DrinkFreqCategory'] = df_final['DrinkDaysPerMonth'].apply(categorize_drink_days)

# ‚úÖ Ordre des cat√©gories
categorie_order = ['Aucun', 'Rare', 'Occasionnel', 'R√©gulier', 'Fr√©quent', 'Quotidien', 'Inconnu']
df_final['DrinkFreqCategory'] = pd.Categorical(df_final['DrinkFreqCategory'], categories=categorie_order, ordered=True)

# ‚úÖ Compter les valeurs
drink_counts = df_final['DrinkFreqCategory'].value_counts().reindex(categorie_order).reset_index()
drink_counts.columns = ['DrinkFreqCategory', 'count']

# ‚úÖ Affichage graphique
fig = px.bar(
    drink_counts,
    x='DrinkFreqCategory',
    y='count',
    text='count',
    labels={'DrinkFreqCategory': 'Cat√©gorie de consommation', 'count': 'Nombre de personnes'},
    title='Fr√©quence de consommation d‚Äôalcool (par mois)'
)

fig.update_traces(textposition='outside')
fig.update_layout(yaxis_title='Nombre de r√©pondants', xaxis_title='Fr√©quence', uniformtext_minsize=8)
fig.show()

df_final['DrinkFreqCategory']

"""##### TimesSunburned"""

import seaborn as sns
import matplotlib.pyplot as plt


# Mappage en 4 fr√©quences
frequency_map_4 = {
    -1: "Aucune r√©ponse / Manquant",
    0: "Jamais",
    1: "Rarement", 2: "Rarement", 3: "Rarement",
    4: "Souvent", 5: "Souvent", 6: "Souvent", 7: "Souvent", 8: "Souvent", 9: "Souvent", 10: "Souvent",
    11: "Tr√®s souvent", 12: "Tr√®s souvent", 14: "Tr√®s souvent", 15: "Tr√®s souvent", 16: "Tr√®s souvent", 20: "Tr√®s souvent",
    25: "Tr√®s souvent", 30: "Tr√®s souvent", 36: "Tr√®s souvent", 40: "Tr√®s souvent", 50: "Tr√®s souvent",
    65: "Tr√®s souvent", 90: "Tr√®s souvent", 99: "Tr√®s souvent"
}
# Appliquer le mappage
# Change this line to assign to df_final instead of df
df_final["TimesSunburned_Frequency"] = df_final["TimesSunburned"].map(frequency_map_4)
# Ordre logique sans la cat√©gorie "manquante"
frequency_order_4 = ["Jamais", "Rarement", "Souvent", "Tr√®s souvent"]
# Compter les fr√©quences et exclure les r√©ponses manquantes
filtered_df_final = df_final[df_final["TimesSunburned_Frequency"] != "Aucune r√©ponse / Manquant"]
frequency_counts_4 = (
    filtered_df_final["TimesSunburned_Frequency"]
    .value_counts()
    .reindex(frequency_order_4)
    .fillna(0)
)
# Tracer le graphique
plt.figure(figsize=(10, 6))
bars = sns.barplot(x=frequency_counts_4.index, y=frequency_counts_4.values, palette='Oranges')
plt.yscale('log')  # √âchelle logarithmique
plt.title("Fr√©quence des coups de soleil (4 cat√©gories, √©chelle log)")
plt.ylabel("Nombre de r√©pondants (log)")
plt.xlabel("Fr√©quence auto-d√©clar√©e des coups de soleil")
# Ajouter les valeurs au-dessus des barres
for index, value in enumerate(frequency_counts_4.values):
    if value > 0:
        bars.text(index, value, f'{int(value)}', ha='center', va='bottom', fontsize=9)
plt.tight_layout()
plt.show()

df_final['TimesSunburned_Frequency']

"""##### AgeGroup"""

df_final['Age'].value_counts()

import pandas as pd
import plotly.express as px

# ‚úÖ Supprimer les colonnes dupliqu√©es
df_final = df_final.loc[:, ~df_final.columns.duplicated()]

# ‚úÖ Nettoyage global
df_final = df_final.replace(-1, pd.NA)

# ‚úÖ Cr√©er la variable AgeGroup si elle n'existe pas
if 'AgeGroup' not in df_final.columns:
    bins = [18, 29, 39, 49, 59, 69, 79, 89, 120]
    labels = ['18-29', '30-39', '40-49', '50-59', '60-69', '70-79', '80-89', '90+']
    df_final = df_final[df_final['Age'].notna() & (df_final['Age'] >= 0)].copy()
    df_final['AgeGroup'] = pd.cut(df_final['Age'], bins=bins, labels=labels, right=True)

# ‚úÖ Variables explicatives et cibles
features = ['AgeGroup']
target_vars = ['CaSkin', 'CaBreast', 'CaProstate', 'CaMelanoma', 'CaCervical']

for target in target_vars:
    if target not in df_final.columns:
        print(f"‚ùå Colonne absente : {target}")
        continue

    print(f"\nüìä Analyse pour : {target}")

    features_present = [f for f in features if f in df_final.columns]
    columns_to_use = features_present + [target]

    df_filtered = df_final[columns_to_use].dropna()
    df_filtered = df_filtered[df_filtered[target].isin([0, 1])]
    df_filtered[target] = df_filtered[target].astype(float)

    for feature in features_present:
        print(f"\nüî∏ {feature} vs {target}")

        cross_tab = pd.crosstab(df_filtered[feature], df_filtered[target], normalize='index') * 100

        for col in [0.0, 1.0]:
            if col not in cross_tab.columns:
                cross_tab[col] = 0
        cross_tab = cross_tab[[0.0, 1.0]]

        df_plot = cross_tab.reset_index().melt(id_vars=feature, var_name=target, value_name='Pourcentage')
        df_plot[target] = df_plot[target].map({0.0: 'Non', 1.0: 'Oui'})

        fig = px.bar(
            df_plot,
            x=feature,
            y='Pourcentage',
            color=target,
            barmode='stack',
            text_auto=True,
            title=f"{target} selon '{feature}'"
        )
        fig.update_layout(xaxis_title=feature, yaxis_title="Pourcentage (%)")
        fig.show()

df_final['AgeGroup']

"""#####CutSkipMeals22"""

df_final['CutSkipMeals2'].value_counts()

import pandas as pd
import plotly.express as px

# V√©rification des colonnes
assert 'CutSkipMeals2' in df_final.columns
assert 'CaSkin' in df_final.columns

# Nettoyage
df_final['CutSkipMeals2'] = pd.to_numeric(df_final['CutSkipMeals2'], errors='coerce')
df_final['CaSkin'] = pd.to_numeric(df_final['CaSkin'], errors='coerce')

def classify_cut(value):
    if pd.isna(value) or value < 0:
        return 'Inconnu'
    elif value in [0, 1, 2, 3, 4]:
        return 'oui'
    elif value == 5:
        return 'non'
    else:
        return 'Inconnu'

df_final['CutSkipMeals2_Cat'] = df_final['CutSkipMeals2'].apply(classify_cut)

# Pr√©paration
df_filtered = df_final[['CutSkipMeals2_Cat', 'CaSkin']].dropna()
df_filtered = df_filtered[df_filtered['CaSkin'].isin([0, 1])]

# Crosstab
cross_tab = pd.crosstab(df_filtered['CutSkipMeals2_Cat'], df_filtered['CaSkin'], normalize='index') * 100
for col in [0, 1]:
    if col not in cross_tab.columns:
        cross_tab[col] = 0
cross_tab = cross_tab[[0, 1]]
cross_tab.columns = ['Non', 'Oui']

df_plot = cross_tab.reset_index().melt(id_vars='CutSkipMeals2_Cat', var_name='Cancer', value_name='Pourcentage')

# Graphique
fig = px.bar(
    df_plot,
    x='CutSkipMeals2_Cat',
    y='Pourcentage',
    color='Cancer',
    barmode='stack',
    text='Pourcentage',
    title="Pr√©sence de CaSkin selon la fr√©quence de repas saut√©s"
)
fig.update_layout(
    xaxis_title="Fr√©quence de repas saut√©s (cat√©goris√©e)",
    yaxis_title="Pourcentage (%)",
    legend_title="Cancer"
)
fig.show()

df_final['CutSkipMeals2_Cat']

"""## IV. Test Chii*2

Le test du Chi¬≤ permet de d√©tecter s‚Äôil existe une relation significative entre deux variables cat√©gorielles.Nous l'utilisons ici pour analyser les liens possibles entre certains comportements et le fait d‚Äôavoir eu un cancer.
Une p-value inf√©rieure √† 0.05 indique une d√©pendance statistique entre les variables.
"""

df_final['EverHadCancer']

df_final.columns.tolist()

df_final.duplicated().sum()

df_final['BirthSex']

df_final['EverHadCancer']

df = df.loc[:, ~df.T.duplicated()].copy()
df = df.loc[:, ~df.columns.duplicated()].copy()

# Liste des colonnes √† v√©rifier
cols = ['FreqGoProvider', 'HadTest3_SpecificDisease', 'HadTest3_Prenatal',
        'ReasonTest_DocRec','ReasonTest_DiseaseRisk', 'Fruit2', 'Vegetables2',
        'CutSkipMeals2_cat', 'LackTransportation2', 'DiffPayMedBills', 'SmokeNow',
        'MarijuanaUseReason', 'DocTalkLDCT', 'DocTellColorectalTests2', 'FamilyEverHadCancer2']

# Ne garde que les colonnes qui existent r√©ellement dans df_final
cols_existantes = [col for col in cols if col in df_final.columns]

# Ajoute la cible
cols_existantes += ['EverHadCancer']

# Applique le filtre sur les colonnes valides
df_correlation = df_final[(df_final[cols_existantes] != -1).all(axis=1)]

from scipy.stats import chi2_contingency
import pandas as pd

def chi2_analysis(df, target_col, feature_cols):
    results = []

    for col in feature_cols:
        # Filtrer les -1 dans les deux colonnes
        subset = df[(df[col] != -1) & (df[target_col] != -1)][[target_col, col]]

        # V√©rification qu'on a au moins 2 modalit√©s dans chaque colonne
        if subset[col].nunique() > 1 and subset[target_col].nunique() > 1:
            table = pd.crosstab(subset[col], subset[target_col])
            chi2, p, dof, expected = chi2_contingency(table)

            result = {
                "Variable": col,
                "Chi2": chi2,
                "p-value": p,
                "D√©pendance": "Oui" if p < 0.05 else "Non"
            }
            results.append(result)
        else:
            results.append({
                "Variable": col,
                "Chi2": None,
                "p-value": None,
                "D√©pendance": "Donn√©es insuffisantes"
            })

    return pd.DataFrame(results)

def remove_duplicate_columns(df):
    """
    Supprime les colonnes dupliqu√©es d'un DataFrame en gardant la premi√®re occurrence.

    Param√®tres :
    df (pd.DataFrame) : Le DataFrame √† nettoyer.

    Retourne :
    pd.DataFrame : Un nouveau DataFrame sans colonnes dupliqu√©es.
    """
    return df.loc[:, ~df.columns.duplicated()]

df_final = remove_duplicate_columns(df_final)

cols_to_test = [
    'FreqGoProvider', 'HadTest3_SpecificDisease', 'HadTest3_Prenatal',
    'ReasonTest_DocRec','ReasonTest_DiseaseRisk', 'Fruit2', 'Vegetables2',
    'CutSkipMeals2', 'LackTransportation2', 'DiffPayMedBills', 'SmokeNow',
    'MarijuanaUseReason', 'DocTalkLDCT', 'DocTellColorectalTests2','FamilyEverHadCancer2'
]

# Re-create df_final and remove duplicate columns
df_final = pd.concat([df_health_Care, df_Genetic_Testing, df_overall_health, df_Environment_Health, df_financial_concern, df_Health_Nutrition, df_alcool, df_tabacco_product, df_diagnosed_cancer, df_Household, df_Person_w], axis=1)
df_final = df_final.loc[:,~df_final.columns.duplicated()]


# Filter cols_to_test to include only columns present in df_final
cols_to_test = [col for col in cols_to_test if col in df_final.columns]

chi2_results = chi2_analysis(df_final, 'EverHadCancer', cols_to_test)
chi2_results

"""### Top 5 des cancers les plus fr√©quents

###### Pour orienter notre analyse sur les types de cancer les plus significatifs, nous avons s√©lectionn√© les 5 formes de cancer comptant le plus grand nombre de cas d√©clar√©s dans notre dataset. Cette √©tape permet de focaliser l'√©tude sur les pathologies les plus repr√©sent√©es, facilitant ainsi une analyse plus pertinente des facteurs associ√©s.
"""

df3 = pd.concat([df_diagnosed_cancer, df_Person_w], axis=1)
df3

df3.info()

# Liste des colonnes de type cancer (selon ton message)
cancer_cols = [
    'CaBladder', 'CaBone', 'CaBreast', 'CaBrain', 'CaCervical', 'CaColon',
    'CaEndometrial', 'CaEye', 'CaHeadNeck', 'CaLeukemia', 'CaLiver', 'CaLung',
    'CaHodgkins', 'CaNonHodgkin', 'CaMelanoma', 'CaMultMyeloma', 'CaOral',
    'CaOvarian', 'CaPancreatic', 'CaPharyngeal', 'CaProstate', 'CaRectal',
    'CaRenal', 'CaSkin', 'CaStomach', 'CaTesticular', 'CaThyroid'
]

# Compter le nombre de cas (valeur == 1)
cancer_counts = {col: (df[col] == 1).sum() for col in cancer_cols}

# Convertir en DataFrame et trier
cancer_df = pd.DataFrame.from_dict(cancer_counts, orient='index', columns=['Nombre_de_cas'])
cancer_df_sorted = cancer_df.sort_values(by='Nombre_de_cas', ascending=False)

# Afficher les 5 cancers les plus fr√©quents
top_5_cancers = cancer_df_sorted.head(5)
print(top_5_cancers)

"""### S√©lection des variables li√©es aux habitudes de vie

###### Dans cette section, nous avons s√©lectionn√© un ensemble de variables issues du questionnaire initial, centr√©es sur les habitudes de vie des participants. L‚Äôobjectif est d‚Äôidentifier des facteurs potentiellement li√©s au mode de vie (alimentation, sommeil, stress, activit√© physique, consommation d‚Äôalcool, situation familiale et conditions √©conomiques). Ces variables serviront de base √† nos futures analyses exploratoires et statistiques pour mieux comprendre leur lien possible avec l‚Äô√©tat de sant√© g√©n√©ral ou le risque de cancer.
"""

df_Variable= ['CutSkipMeals2_cat',
    'DiffPayMedBills', 'SmokeNow',
    'MedConditions_Diabetes', 'MedConditions_HighBP', 'MedConditions_HeartCondition',
    'MedConditions_LungDisease', 'MedConditions_Depression', 'GeneralHealth', 'HealthLimits_Pain', 'Nervous',
    'IncomeRanges', 'Education', 'Fruit2', 'Vegetables2', 'TimesSunburned_Frequency', 'DrinkFreqCategory',
    'ChildrenInHH', 'TotalHousehold','AgeGroup', 'SleepCategory','Mexican','PuertoRican','Cuban','OthHisp','Hisp_Cat','White','Black','AmerInd','AsInd',
    'Chinese','Filipino','Japanese','Korean','Vietnamese','OthAsian','OthPacIsl'
]

"""## V. Crosstab & barplots : visualisation conditionnelle des diff√©rents types de Cancer

##### Nous avons automatis√© l'exploration bivari√©e entre nos variables cibles (`CaSkin`, `CaBreast`, `CaProstate`, `CaMelanoma` et `CaCervical`) et une s√©rie de variables explicatives (habitudes de vie, nutrition, sant√© mentale, etc.). Pour chaque variable, un graphique en barres empil√©es montre la proportion de cas positifs et n√©gatifs de cancer de la peau, permettant ainsi d‚Äôidentifier visuellement des tendances int√©ressantes.

"""

# Variables (d√©finies selon l'ordre logique pour le preprocessor)
# 3. D√©finir les variables
binary_vars = ['CutSkipMeals2','DiffPayMedBills','SmokeNow', 'MedConditions_Diabetes','MedConditions_HighBP','MedConditions_HeartCondition',
                 'MedConditions_LungDisease', 'MedConditions_Depression', 'BirthSex', 'FamilyEverHadCancer2']

ordinal_vars = ['GeneralHealth','HealthLimits_Pain', 'Nervous', 'IncomeRanges', 'Education']
string_categorical_vars = [
    'Mexican','PuertoRican','Cuban','OthHisp','Hisp_Cat','White','Black','AmerInd','AsInd',
    'Chinese','Filipino','Japanese','Korean','Vietnamese','OthAsian','OthPacIsl'
]
continuous_vars = [
    'Fruit2', 'Vegetables2', 'TimesStrengthTraining', 'Drink_nb_PerMonth',
    'ChildrenInHH', 'TotalHousehold', 'TimesSunburned', 'BMI', 'Age', 'SleepWeekdayHr'
]
target = 'EverHadCancer'

print(df_final.columns[df_final.columns.duplicated()])

df_final = df_final.loc[:, ~df_final.columns.duplicated()]

"""### 1. Les variables Binaire"""

import pandas as pd
import plotly.express as px

# ‚úÖ Variables explicatives communes
features = [
    'CutSkipMeals2_cat',
    'DiffPayMedBills', 'SmokeNow',
    'MedConditions_Diabetes', 'MedConditions_HighBP', 'MedConditions_HeartCondition',
    'MedConditions_LungDisease', 'MedConditions_Depression'
]

# ‚úÖ Liste des colonnes cibles (cancers)
target_vars = ['CaSkin', 'CaBreast', 'CaProstate', 'CaMelanoma', 'CaCervical']

for target in target_vars:
    if target not in df_final.columns:
        print(f"‚ùå Colonne absente : {target}")
        continue

    print(f"\nüìä Analyse pour : {target}")

    # ‚úÖ V√©rifie que toutes les colonnes sont l√†
    features_present = [f for f in features if f in df_final.columns]
    columns_to_use = features_present + [target]

    # ‚úÖ Nettoyage (-1 ‚Üí NaN, drop des lignes incompl√®tes)
    df_filtered = df_final[columns_to_use].replace(-1, pd.NA).dropna()

    # ‚úÖ Supprimer les lignes contenant 'Inconnu'
    for col in features_present:
        df_filtered = df_filtered[df_filtered[col] != 'Inconnu']

    # ‚úÖ Forcer les valeurs cibles √† 0.0 / 1.0 uniquement (si d'autres valeurs existent)
    df_filtered = df_filtered[df_filtered[target].isin([0, 1, 0.0, 1.0])]
    df_filtered[target] = df_filtered[target].astype(float)

    for feature in features_present:
        print(f"\nüî∏ {feature} vs {target}")

        # Crosstab normalis√©
        cross_tab = pd.crosstab(df_filtered[feature], df_filtered[target], normalize='index') * 100

        # Forcer les colonnes 0.0 et 1.0 m√™me si absentes
        for col in [0.0, 1.0]:
            if col not in cross_tab.columns:
                cross_tab[col] = 0
        cross_tab = cross_tab[[0.0, 1.0]]  # ordre fixe

        # ‚úÖ Pr√©paration graphique
        cross_tab.index.name = feature
        df_plot = cross_tab.reset_index().melt(id_vars=feature, var_name=target, value_name='Pourcentage')
        df_plot[target] = df_plot[target].map({0.0: 'Non', 1.0: 'Oui'})

        fig = px.bar(
            df_plot,
            x=feature,
            y='Pourcentage',
            color=target,
            barmode='stack',
            text_auto=True,
            title=f"{target} selon '{feature}'"
        )
        fig.show()

"""### 2. Les Variables Cat√©gorielles Ordinales"""

import pandas as pd
import plotly.express as px

# ‚úÖ Variables explicatives communes
features = [
    'GeneralHealth', 'HealthLimits_Pain', 'Nervous',
    'IncomeRanges', 'Education'
]

# ‚úÖ Liste des colonnes cibles (cancers)
target_vars = ['CaSkin', 'CaBreast', 'CaProstate', 'CaMelanoma', 'CaCervical']

for target in target_vars:
    if target not in df_final.columns:
        print(f"‚ùå Colonne absente : {target}")
        continue

    print(f"\nüìä Analyse pour : {target}")

    # ‚úÖ V√©rifie que toutes les colonnes sont l√†
    features_present = [f for f in features if f in df_final.columns]
    columns_to_use = features_present + [target]

    # ‚úÖ Nettoyage (-1 ‚Üí NaN, drop des lignes incompl√®tes)
    df_filtered = df_final[columns_to_use].replace(-1, pd.NA).dropna()

    # ‚úÖ Forcer les valeurs cibles √† 0.0 / 1.0 uniquement (si d'autres valeurs existent)
    df_filtered = df_filtered[df_filtered[target].isin([0, 1, 0.0, 1.0])]
    df_filtered[target] = df_filtered[target].astype(float)

    for feature in features_present:
        print(f"\nüî∏ {feature} vs {target}")

        # Crosstab normalis√©
        cross_tab = pd.crosstab(df_filtered[feature], df_filtered[target], normalize='index') * 100

        # Forcer les colonnes 0.0 et 1.0 m√™me si absentes
        for col in [0.0, 1.0]:
            if col not in cross_tab.columns:
                cross_tab[col] = 0
        cross_tab = cross_tab[[0.0, 1.0]]  # ordre fixe

        # ‚úÖ Pr√©paration graphique
        cross_tab.index.name = feature
        df_plot = cross_tab.reset_index().melt(id_vars=feature, var_name=target, value_name='Pourcentage')
        df_plot[target] = df_plot[target].map({0.0: 'Non', 1.0: 'Oui'})

        fig = px.bar(
            df_plot,
            x=feature,
            y='Pourcentage',
            color=target,
            barmode='stack',
            text_auto=True,
            title=f"{target} selon '{feature}'"
        )
        fig.show()

"""### 3. Variables Cat√©gorielles Textuelles"""

import pandas as pd
import plotly.express as px

# ‚úÖ Variables explicatives communes
features = [
    'Mexican','PuertoRican','Cuban','OthHisp','Hisp_Cat','White','Black','AmerInd','AsInd',
    'Chinese','Filipino','Japanese','Korean','Vietnamese','OthAsian','OthPacIsl'
]

# ‚úÖ Liste des colonnes cibles (cancers)
target_vars = ['CaSkin', 'CaBreast', 'CaProstate', 'CaMelanoma', 'CaCervical']

for target in target_vars:
    if target not in df_final.columns:
        print(f"‚ùå Colonne absente : {target}")
        continue

    print(f"\nüìä Analyse pour : {target}")

    # ‚úÖ V√©rifie que toutes les colonnes sont l√†
    features_present = [f for f in features if f in df_final.columns]
    columns_to_use = features_present + [target]

    # ‚úÖ Nettoyage (-1 ‚Üí NaN, drop des lignes incompl√®tes)
    df_filtered = df_final[columns_to_use].replace(-1, pd.NA).dropna()

    # ‚úÖ Forcer les valeurs cibles √† 0.0 / 1.0 uniquement (si d'autres valeurs existent)
    df_filtered = df_filtered[df_filtered[target].isin([0, 1, 0.0, 1.0])]
    df_filtered[target] = df_filtered[target].astype(float)

    for feature in features_present:
        print(f"\nüî∏ {feature} vs {target}")

        # Crosstab normalis√©
        cross_tab = pd.crosstab(df_filtered[feature], df_filtered[target], normalize='index') * 100

        # Forcer les colonnes 0.0 et 1.0 m√™me si absentes
        for col in [0.0, 1.0]:
            if col not in cross_tab.columns:
                cross_tab[col] = 0
        cross_tab = cross_tab[[0.0, 1.0]]  # ordre fixe

        # ‚úÖ Pr√©paration graphique
        cross_tab.index.name = feature
        df_plot = cross_tab.reset_index().melt(id_vars=feature, var_name=target, value_name='Pourcentage')
        df_plot[target] = df_plot[target].map({0.0: 'Non', 1.0: 'Oui'})

        fig = px.bar(
            df_plot,
            x=feature,
            y='Pourcentage',
            color=target,
            barmode='stack',
            text_auto=True,
            title=f"{target} selon '{feature}'"
        )
        fig.show()

"""### 4. Variables Num√©riques Continues"""

import pandas as pd
import plotly.express as px

# ‚úÖ Variables explicatives communes
features = [
    'Fruit2', 'Vegetables2', 'TimesSunburned_Frequency', 'DrinkFreqCategory',
    'ChildrenInHH', 'TotalHousehold','AgeGroup', 'SleepCategory'
]

# ‚úÖ Liste des colonnes cibles (cancers)
target_vars = ['CaSkin', 'CaBreast', 'CaProstate', 'CaMelanoma', 'CaCervical']

for target in target_vars:
    if target not in df_final.columns:
        print(f"‚ùå Colonne absente : {target}")
        continue

    print(f"\nüìä Analyse pour : {target}")

    # ‚úÖ V√©rifie que toutes les colonnes sont l√†
    features_present = [f for f in features if f in df_final.columns]
    columns_to_use = features_present + [target]

    # ‚úÖ Nettoyage (-1 ‚Üí NaN, drop des lignes incompl√®tes)
    df_filtered = df_final[columns_to_use].replace(-1, pd.NA).dropna()

    # ‚úÖ Forcer les valeurs cibles √† 0.0 / 1.0 uniquement (si d'autres valeurs existent)
    df_filtered = df_filtered[df_filtered[target].isin([0, 1, 0.0, 1.0])]
    df_filtered[target] = df_filtered[target].astype(float)

    for feature in features_present:
        print(f"\nüî∏ {feature} vs {target}")

        # Crosstab normalis√©
        cross_tab = pd.crosstab(df_filtered[feature], df_filtered[target], normalize='index') * 100

        # Forcer les colonnes 0.0 et 1.0 m√™me si absentes
        for col in [0.0, 1.0]:
            if col not in cross_tab.columns:
                cross_tab[col] = 0
        cross_tab = cross_tab[[0.0, 1.0]]  # ordre fixe

        # ‚úÖ Pr√©paration graphique
        cross_tab.index.name = feature
        df_plot = cross_tab.reset_index().melt(id_vars=feature, var_name=target, value_name='Pourcentage')
        df_plot[target] = df_plot[target].map({0.0: 'Non', 1.0: 'Oui'})

        fig = px.bar(
            df_plot,
            x=feature,
            y='Pourcentage',
            color=target,
            barmode='stack',
            text_auto=True,
            title=f"{target} selon '{feature}'"
        )
        fig.show()

"""## VI. Mod√©lisation Pr√©dictive avec Random Forest

Dans cette partie, nous utilisons l‚Äôalgorithme Random Forest pour pr√©dire la survenue d‚Äôun cancer √† partir de diff√©rentes variables li√©es √† la sant√©, aux habitudes de vie et aux facteurs socio-√©conomiques. Ce mod√®le permet non seulement d‚Äô√©valuer les performances de pr√©diction, mais aussi d‚Äôidentifier les variables les plus influentes dans le processus de d√©cision.

###A. Quel Mod√®le Pr√©dit le Mieux ? Test avec PyCaret
"""

pip install pycaret

"""#### PyCarret"""

from pycaret.classification import *

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler
from sklearn.metrics import accuracy_score, recall_score, precision_score

# 1. Charger les donn√©es
df = pd.read_csv('df_final.csv')

# 2. Nettoyage : remplacer les -1 par NaN, supprimer les colonnes dupliqu√©es
df.replace(-1, pd.NA, inplace=True)
df = df.loc[:, ~df.columns.duplicated()]

# 3. D√©finir les variables
binary_vars = ['CutSkipMeals2','DiffPayMedBills','SmokeNow', 'MedConditions_Diabetes','MedConditions_HighBP','MedConditions_HeartCondition',
                 'MedConditions_LungDisease', 'MedConditions_Depression', 'BirthSex', 'FamilyEverHadCancer2']

ordinal_vars = ['GeneralHealth','HealthLimits_Pain', 'Nervous', 'IncomeRanges', 'Education']
string_categorical_vars = [
    'Mexican','PuertoRican','Cuban','OthHisp','Hisp_Cat','White','Black','AmerInd','AsInd',
    'Chinese','Filipino','Japanese','Korean','Vietnamese','OthAsian','OthPacIsl'
]
continuous_vars = [
    'Fruit2', 'Vegetables2', 'TimesStrengthTraining', 'Drink_nb_PerMonth',
    'ChildrenInHH', 'TotalHousehold', 'TimesSunburned', 'BMI', 'Age', 'SleepWeekdayHr'
]
target = 'EverHadCancer'

features = binary_vars + ordinal_vars + continuous_vars + string_categorical_vars

# 5. Nettoyage final
df_final = df_final[features + [target]].dropna()

X = df_final[features].copy()
y = df_final[target]

# 6. Encodage manuel

# Ordinal encoding
encoder_ord = OrdinalEncoder()
X[ordinal_vars] = encoder_ord.fit_transform(X[ordinal_vars])

# One-hot encoding
encoder_ohe = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')
ohe_array = encoder_ohe.fit_transform(X[string_categorical_vars])
ohe_columns = encoder_ohe.get_feature_names_out(string_categorical_vars)
df_ohe = pd.DataFrame(ohe_array, columns=ohe_columns, index=X.index)

# Standardisation (optionnelle)
scaler = StandardScaler()
X[continuous_vars] = scaler.fit_transform(X[continuous_vars])

# Concat√©ner toutes les features finales
X_final = pd.concat([
    X[binary_vars + ordinal_vars + continuous_vars],
    df_ohe
], axis=1)

# 7. Split & entra√Ænement
X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.2, random_state=42)

model = RandomForestClassifier(n_estimators=1073, random_state=142)
model.fit(X_train, y_train)

# 6. Initialiser l‚Äôenvironnement PyCaret
# 6. Initialiser l‚Äôenvironnement PyCaret
from pycaret.classification import *
clf_setup = setup(
    data=df_final,
    target='EverHadCancer',
    session_id=142,
    preprocess=True,  # encodage, normalisation, etc.
    ignore_features=[],  # tu peux exclure certaines colonnes si n√©cessaire
    categorical_features=None  # optionnel si PyCaret ne d√©tecte pas tout seul
)

# 7. Comparer tous les mod√®les disponibles
best_model = compare_models()

# 8. Afficher les m√©triques du meilleur mod√®le
print("Mod√®le s√©lectionn√© :", best_model)

# 9. √âvaluer graphiquement ce mod√®le
evaluate_model(best_model)

# 10. Interpr√©ter l‚Äôimportance des variables
plot_model(best_model, plot='feature')

# 11. (Optionnel) Sauvegarder le mod√®le
# save_model(best_model, 'modele_pycaret_cancer')

# 4. Comparer les mod√®les et extraire les 3 meilleurs
top3_models = compare_models(n_select=3)

# 5. Afficher le top 3
print("üèÜ Top 3 des mod√®les s√©lectionn√©s par PyCaret :")
for i, model in enumerate(top3_models, start=1):
    print(f"{i}. {model}")

# 6. √âvaluer le meilleur mod√®le graphiquement
evaluate_model(top3_models[0])  # Premier du classement

# 7. Importance des variables pour le meilleur mod√®le
plot_model(top3_models[0], plot='feature')

"""##### Conclusion : Les trois algorithme qui ressort sont :

    1. Random Forest
    2.LogisticRegression
    3.RidgeClassifier

### B. Mise en Pratiques des r√©sultat obtenue: √âvaluation de notre mod√©le

on va mettre en pratique un Random Forest et on ajoute un SMOTE

#### Random Forest
"""

import sklearn
print(sklearn.__version__)

import pandas as pd
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline
from imblearn.pipeline import Pipeline as ImbPipeline
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler

# 1. Charger les donn√©es
df = pd.read_csv('df_final.csv')

# 2. Nettoyage : remplacer les -1 par NaN, supprimer les colonnes dupliqu√©es
df.replace(-1, pd.NA, inplace=True)

# 3. D√©finir les variables
binary_vars = ['CutSkipMeals2','DiffPayMedBills','SmokeNow', 'MedConditions_Diabetes','MedConditions_HighBP','MedConditions_HeartCondition',
                 'MedConditions_LungDisease', 'MedConditions_Depression', 'BirthSex', 'FamilyEverHadCancer2']

ordinal_vars = ['GeneralHealth','HealthLimits_Pain', 'Nervous', 'IncomeRanges', 'Education']
string_categorical_vars = [
    'Mexican','PuertoRican','Cuban','OthHisp','Hisp_Cat','White','Black','AmerInd','AsInd',
    'Chinese','Filipino','Japanese','Korean','Vietnamese','OthAsian','OthPacIsl'
]
continuous_vars = [
    'Fruit2', 'Vegetables2', 'TimesStrengthTraining', 'Drink_nb_PerMonth',
    'ChildrenInHH', 'TotalHousehold', 'TimesSunburned', 'BMI', 'Age', 'SleepWeekdayHr'
]
target = 'EverHadCancer'


# 4. Suppression des colonnes dupliqu√©es
df = df.loc[:, ~df.columns.duplicated()]

# 6. S√©lection des variables et suppression des lignes incompl√®tes
features = binary_vars + ordinal_vars + continuous_vars + string_categorical_vars
df_model = df[features + [target]].dropna()

X = df_model[features]
y = df_model[target]

# 9. Split des donn√©es
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=142)

# 7. Pr√©traitement
# Pr√©processeur avec ColumnTransformer
# L'ordre des transformations ici D√âFINIT l'ordre de sortie des features dans le pipeline
# et donc l'ordre attendu par le mod√®le final
transformers = [
    ('ord', OrdinalEncoder(), ordinal_vars),
    ('ohe', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), string_categorical_vars),
    ('scale', StandardScaler(), continuous_vars)
]

preprocessor = ColumnTransformer(
    transformers=transformers,
    remainder='passthrough'  # les variables binaires passent sans transformation
)

# Cr√©er le pipeline complet, y compris SMOTE
# SMOTE est appliqu√© APR√àS le pr√©processeur mais AVANT le classificateur.
pipeline = Pipeline([
    ('preprocess', preprocessor),
    ('smote', SMOTE(random_state=142)), # Ajout de l'√©tape SMOTE
    ('model', RandomForestClassifier(n_estimators=1000, random_state=142))
])

# entrainement du mod√®le
pipeline.fit(X_train, y_train)

# √âvaluation du mod√®le
y_pred = pipeline.predict(X_test)
print("Random Forest Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

import matplotlib.pyplot as plt
model_rf = pipeline.named_steps['model']
importances = model_rf.feature_importances_

# R√©cup√©rer les noms des features finales
ohe = pipeline.named_steps['preprocess'].named_transformers_['ohe']
ohe_names = ohe.get_feature_names_out(string_categorical_vars)
final_features = ordinal_vars + list(ohe_names) + continuous_vars + binary_vars

# Plot
importances_df = pd.DataFrame({'Feature': final_features, 'Importance': importances})
top = importances_df.sort_values('Importance', ascending=False).head(15)

plt.figure(figsize=(10,6))
plt.barh(top['Feature'][::-1], top['Importance'][::-1])
plt.title("Top 15 Features Importantes")
plt.tight_layout()
plt.show()

# Pipeline sans SMOTE
pipeline_nosmote = Pipeline([
    ('preprocess', preprocessor),
    ('model', RandomForestClassifier(n_estimators=100, random_state=142))
])

# Pipeline avec SMOTE
pipeline_smote = ImbPipeline([
    ('preprocess', preprocessor),
    ('smote', SMOTE(random_state=142)),
    ('model', RandomForestClassifier(n_estimators=100, random_state=142))
])

pipeline_nosmote.fit(X_train, y_train)
y_pred_nosmote = pipeline_nosmote.predict(X_test)

pipeline_smote.fit(X_train, y_train)
y_pred_smote = pipeline_smote.predict(X_test)

print("üìä R√©sultats sans SMOTE :")
print("Accuracy :", accuracy_score(y_test, y_pred_nosmote))
print("Recall :", recall_score(y_test, y_pred_nosmote, average='macro'))
print("Precision :", precision_score(y_test, y_pred_nosmote, average='macro'))

print("\nüìä R√©sultats avec SMOTE :")
print("Accuracy :", accuracy_score(y_test, y_pred_smote))
print("Recall :", recall_score(y_test, y_pred_smote, average='macro'))
print("Precision :", precision_score(y_test, y_pred_smote, average='macro'))

# 8. Matrices de confusion
import seaborn as sns
from sklearn.metrics import confusion_matrix

fig, axs = plt.subplots(1, 2, figsize=(12, 5))
sns.heatmap(confusion_matrix(y_test, y_pred_nosmote), annot=True, fmt='d', ax=axs[0])
axs[0].set_title('Sans SMOTE')
axs[0].set_xlabel('Pr√©dit')
axs[0].set_ylabel('R√©el')

sns.heatmap(confusion_matrix(y_test, y_pred_smote), annot=True, fmt='d', ax=axs[1])
axs[1].set_title('Avec SMOTE')
axs[1].set_xlabel('Pr√©dit')
axs[1].set_ylabel('R√©el')

plt.tight_layout()
plt.show()

"""Conclusion: on Obtient un r√©sultat Satisfaisant mais on peut faire mieux, en faisant un r√©-√©quilibrage manuel afin d'botenir un meilleur r√©sultat.

### C.√âquilibrage Manuellement de l'√©chantillonage

##### Pycaret
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, recall_score, precision_score
import matplotlib.pyplot as plt

# 1. Chargement des donn√©es
df = pd.read_csv('df_final.csv')

# 2. Nettoyage : remplacer les -1 par NaN
df.replace(-1, pd.NA, inplace=True)
df = df.loc[:, ~df.columns.duplicated()]

# 3. Filtrage des colonnes utiles
binary_vars = ['CutSkipMeals2','DiffPayMedBills','SmokeNow', 'MedConditions_Diabetes','MedConditions_HighBP','MedConditions_HeartCondition',
                 'MedConditions_LungDisease', 'MedConditions_Depression', 'BirthSex', 'FamilyEverHadCancer2']

ordinal_vars = ['GeneralHealth','HealthLimits_Pain', 'Nervous', 'IncomeRanges', 'Education']
string_categorical_vars = [
    'Mexican','PuertoRican','Cuban','OthHisp','Hisp_Cat','White','Black','AmerInd','AsInd',
    'Chinese','Filipino','Japanese','Korean','Vietnamese','OthAsian','OthPacIsl'
]
continuous_vars = [
    'Fruit2', 'Vegetables2', 'TimesStrengthTraining', 'Drink_nb_PerMonth',
    'ChildrenInHH', 'TotalHousehold', 'TimesSunburned', 'BMI', 'Age', 'SleepWeekdayHr'
]
target = 'EverHadCancer'


# 5. √âquilibrage : undersampling
df_cancer = df[df[target] == 1]
df_noncancer = df[df[target] == 0].sample(n=len(df_cancer), random_state=142)
df_balanced = pd.concat([df_cancer, df_noncancer]).sample(frac=1, random_state=142)

# 6. S√©lection des variables et suppression des lignes incompl√®tes
features = binary_vars + ordinal_vars + continuous_vars + string_categorical_vars

# 7. Nettoyage final
df_model = df_balanced[features + [target]].dropna()

X = df_model[features]
y = df_model[target]

# 9. Split des donn√©es
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 9. Pr√©traitement
# Pr√©processeur avec ColumnTransformer
# L'ordre des transformations ici D√âFINIT l'ordre de sortie des features dans le pipeline
# et donc l'ordre attendu par le mod√®le final
transformers = [
    ('ord', OrdinalEncoder(), ordinal_vars),
    ('ohe', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), string_categorical_vars),
    ('scale', StandardScaler(), continuous_vars)
]

preprocessor = ColumnTransformer(
    transformers=transformers,
    remainder='passthrough'  # les variables binaires passent sans transformation
)

# 10.Cr√©er le pipeline complet,
pipeline = Pipeline([
    ('preprocess', preprocessor),
    ('model', RandomForestClassifier(n_estimators=100, random_state=142))
])

# 11.entrainement du mod√®le
pipeline.fit(X_train, y_train)

# 12.√âvaluation du mod√®le
y_pred = pipeline.predict(X_test)
print("Random Forest Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# 6. Initialiser l‚Äôenvironnement PyCaret
# 6. Initialiser l‚Äôenvironnement PyCaret
from pycaret.classification import *
clf_setup = setup(
    data=df_final,
    target='EverHadCancer',
    session_id=142,
    preprocess=True,  # encodage, normalisation, etc.
    ignore_features=[],  # tu peux exclure certaines colonnes si n√©cessaire
    categorical_features=None  # optionnel si PyCaret ne d√©tecte pas tout seul
)

# 7. Comparer tous les mod√®les disponibles
best_model = compare_models()

# 8. Afficher les m√©triques du meilleur mod√®le
print("Mod√®le s√©lectionn√© :", best_model)

# 9. √âvaluer graphiquement ce mod√®le
evaluate_model(best_model)

# 10. Interpr√©ter l‚Äôimportance des variables
plot_model(best_model, plot='feature')

# 11. (Optionnel) Sauvegarder le mod√®le
# save_model(best_model, 'modele_pycaret_cancer')

# 4. Comparer les mod√®les et extraire les 3 meilleurs
top3_models = compare_models(n_select=3)

# 5. Afficher le top 3
print("üèÜ Top 3 des mod√®les s√©lectionn√©s par PyCaret :")
for i, model in enumerate(top3_models, start=1):
    print(f"{i}. {model}")

# 6. √âvaluer le meilleur mod√®le graphiquement
evaluate_model(top3_models[0])  # Premier du classement

# 7. Importance des variables pour le meilleur mod√®le
plot_model(top3_models[0], plot='feature')

"""#### **Random Forest**"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# 1. Chargement des donn√©es
df = pd.read_csv('df_final.csv')

# 2. Nettoyage : remplacer les -1 par NaN
df.replace(-1, pd.NA, inplace=True)
df = df.loc[:, ~df.columns.duplicated()]

# 3. Filtrage des colonnes utiles
binary_vars = ['CutSkipMeals2','DiffPayMedBills','SmokeNow', 'MedConditions_Diabetes','MedConditions_HighBP','MedConditions_HeartCondition',
                 'MedConditions_LungDisease', 'MedConditions_Depression', 'BirthSex', 'FamilyEverHadCancer2']

ordinal_vars = ['GeneralHealth','HealthLimits_Pain', 'Nervous', 'IncomeRanges', 'Education']

string_categorical_vars = [
    'Mexican','PuertoRican','Cuban','OthHisp','Hisp_Cat','White','Black','AmerInd','AsInd',
    'Chinese','Filipino','Japanese','Korean','Vietnamese','OthAsian','OthPacIsl'
]
continuous_vars = [
    'Fruit2', 'Vegetables2', 'TimesStrengthTraining', 'Drink_nb_PerMonth',
    'ChildrenInHH', 'TotalHousehold', 'TimesSunburned', 'BMI', 'Age', 'SleepWeekdayHr'
]
target = 'EverHadCancer'

# 5. √âquilibrage : undersampling
df_cancer = df[df[target] == 1]
df_noncancer = df[df[target] == 0].sample(n=len(df_cancer), random_state=142)
df_balanced = pd.concat([df_cancer, df_noncancer]).sample(frac=1, random_state=142).reset_index(drop=True)

print("‚úÖ Distribution √©quilibr√©e :")
print(df_balanced[target].value_counts())

# 6. S√©lection des variables et suppression des lignes incompl√®tes
features = binary_vars + ordinal_vars + continuous_vars + string_categorical_vars

# 7. Nettoyage final
df_model = df_balanced[features + [target]].dropna()

X = df_model[features]
y = df_model[target]

# 8. Split des donn√©es
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=142)

# 9. Pr√©traitement
# Pr√©processeur avec ColumnTransformer
# L'ordre des transformations ici D√âFINIT l'ordre de sortie des features dans le pipeline
# et donc l'ordre attendu par le mod√®le final
transformers = [
    ('ord', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), ordinal_vars), # Added handle_unknown and unknown_value
    ('ohe', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), string_categorical_vars),
    ('scale', StandardScaler(), continuous_vars)
]

preprocessor = ColumnTransformer(
    transformers=transformers,
    remainder='passthrough'  # les variables binaires passent sans transformation
)

# 10.Cr√©er le pipeline complet,
pipeline = Pipeline([
    ('preprocess', preprocessor),
    ('model', RandomForestClassifier(n_estimators=100, random_state=142))
])

# 11.entrainement du mod√®le
pipeline.fit(X_train, y_train)

# 12.√âvaluation du mod√®le
y_pred = pipeline.predict(X_test)
print("Random Forest Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline

# Pipeline pour la r√©gression logistique
pipeline_logreg = Pipeline([
    ('preprocess', preprocessor),
    ('model', LogisticRegression(max_iter=1000, random_state=142))
])

# Entra√Æner le mod√®le
pipeline_logreg.fit(X_train, y_train)

# Extraire les coefficients du mod√®le
model_logreg = pipeline_logreg.named_steps['model']
coefficients = model_logreg.coef_[0]

# R√©cup√©rer les noms des variables transform√©es
ohe = pipeline_logreg.named_steps['preprocess'].named_transformers_['ohe']
ohe_names = ohe.get_feature_names_out(string_categorical_vars)

final_features = ordinal_vars + list(ohe_names) + continuous_vars + binary_vars

# Cr√©er DataFrame avec les coefficients
coef_df = pd.DataFrame({
    'Feature': final_features,
    'Importance': coefficients
}).sort_values(by='Importance')

# Affichage
plt.figure(figsize=(8, 10))
plt.barh(coef_df['Feature'], coef_df['Importance'], color='cornflowerblue')
plt.axvline(0, color='black', linewidth=0.8)
plt.title("Importance des variables (r√©gression logistique)")
plt.xlabel("Coefficient")
plt.tight_layout()
plt.show()

"""##### Graphique des coefficients (avec barres n√©gatives ou positives) :"""

# Tracer
#Ci dessous la matrice de confusion

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Exemple : y_true = vraies √©tiquettes, y_pred = pr√©dictions du mod√®le
cm = confusion_matrix(y_test, y_pred)

# Affichage avec seaborn (matrice en couleur)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Pr√©dictions')
plt.ylabel('Vrais')
plt.title('Matrice de confusion')
plt.show()

"""### **D. Cross Validation: Vers une validation robuste**

Une fois les donn√©es √©quilibr√©es avec SMOTE, il devient essentiel de tester la stabilit√© de notre mod√®le.
En effet, les r√©sultats obtenus peuvent d√©pendre du hasard dans le d√©coupage des donn√©es (train/test split).

Nous allons donc utiliser la validation crois√©e (cross-validation), qui consiste √† r√©p√©ter plusieurs fois l'entra√Ænement sur diff√©rentes portions du jeu de donn√©es, afin de fournir une moyenne plus fiable des performances.
"""

from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier

# Pipeline d√©j√† d√©fini avec preprocessing + mod√®le
scores = cross_val_score(pipeline, X, y, cv=5, scoring='accuracy')

print("Scores individuels :", scores)
print("‚úÖ Moyenne de l'accuracy :", scores.mean())

"""##### Equilibrage avec Undersampling"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_validate
from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# 1. Chargement des donn√©es
df = pd.read_csv('df_final.csv')
df.replace(-1, pd.NA, inplace=True)
df = df.loc[:, ~df.columns.duplicated()]
target = 'EverHadCancer'

# 2. Variables
binary_vars = ['CutSkipMeals2','DiffPayMedBills','SmokeNow', 'MedConditions_Diabetes','MedConditions_HighBP','MedConditions_HeartCondition',
                 'MedConditions_LungDisease', 'MedConditions_Depression', 'BirthSex', 'FamilyEverHadCancer2']

ordinal_vars = ['GeneralHealth','HealthLimits_Pain', 'Nervous', 'IncomeRanges', 'Education']
string_categorical_vars = [
    'Mexican','PuertoRican','Cuban','OthHisp','Hisp_Cat','White','Black','AmerInd','AsInd',
    'Chinese','Filipino','Japanese','Korean','Vietnamese','OthAsian','OthPacIsl'
]
continuous_vars = [
    'Fruit2', 'Vegetables2', 'TimesStrengthTraining', 'Drink_nb_PerMonth',
    'ChildrenInHH', 'TotalHousehold', 'TimesSunburned', 'BMI', 'Age', 'SleepWeekdayHr'
]
target = 'EverHadCancer'

features = binary_vars + ordinal_vars + continuous_vars + string_categorical_vars

# 3. √âquilibrage
df_cancer = df[df[target] == 1]
df_noncancer = df[df[target] == 0].sample(n=len(df_cancer), random_state=142)
df_balanced = pd.concat([df_cancer, df_noncancer]).sample(frac=1, random_state=142).reset_index(drop=True)

# 4. S√©lection et nettoyage
df_model = df_balanced[features + [target]].dropna()
X = df_model[features]
y = df_model[target]

# 5. Pr√©processing
preprocessor = ColumnTransformer([
    ('ord', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), ordinal_vars), # Added handle_unknown and unknown_value
    ('ohe', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), string_categorical_vars),
    ('scale', StandardScaler(), continuous_vars)
], remainder='passthrough')

# 6. Pipeline
pipeline = Pipeline([
    ('preprocess', preprocessor),
    ('model', RandomForestClassifier(n_estimators=100, random_state=142))
])

# 7. Validation crois√©e
scores = cross_validate(
    pipeline,
    X, y,
    cv=5,
    scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'],
    return_train_score=False
)

# 8. R√©sultats
print("‚úÖ Moyenne de l'accuracy       :", round(scores['test_accuracy'].mean(), 3))
print("‚úÖ Moyenne du recall (macro)   :", round(scores['test_recall_macro'].mean(), 3))
print("‚úÖ Moyenne de la pr√©cision     :", round(scores['test_precision_macro'].mean(), 3))
print("‚úÖ Moyenne du F1-score         :", round(scores['test_f1_macro'].mean(), 3))
print("‚ÑπÔ∏è  √âcart-type Accuracy         :", round(np.std(scores['test_accuracy']), 3))

"""##### Equilibrage avec SMOTE"""

from sklearn.model_selection import cross_validate
import pandas as pd
import numpy as np
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler

# 1. Chargement
df = pd.read_csv('df_final.csv')
df.replace(-1, pd.NA, inplace=True)
df = df.loc[:, ~df.columns.duplicated()]
target = 'EverHadCancer'

# 2. Variables
binary_vars = ['CutSkipMeals2','DiffPayMedBills','SmokeNow', 'MedConditions_Diabetes','MedConditions_HighBP','MedConditions_HeartCondition',
                 'MedConditions_LungDisease', 'MedConditions_Depression', 'BirthSex', 'FamilyEverHadCancer2']

ordinal_vars = ['GeneralHealth','HealthLimits_Pain', 'Nervous', 'IncomeRanges', 'Education']
string_categorical_vars = [
    'Mexican','PuertoRican','Cuban','OthHisp','Hisp_Cat','White','Black','AmerInd','AsInd',
    'Chinese','Filipino','Japanese','Korean','Vietnamese','OthAsian','OthPacIsl'
]
continuous_vars = [
    'Fruit2', 'Vegetables2', 'TimesStrengthTraining', 'Drink_nb_PerMonth',
    'ChildrenInHH', 'TotalHousehold', 'TimesSunburned', 'BMI', 'Age', 'SleepWeekdayHr'
]

features = binary_vars + ordinal_vars + continuous_vars + string_categorical_vars

# 3. Donn√©es
df_model = df[features + [target]].dropna()
X = df_model[features]
y = df_model[target]

# 4. Pr√©traitement
preprocessor = ColumnTransformer([
    ('ord', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), ordinal_vars), # Added handle_unknown and unknown_value
    ('ohe', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), string_categorical_vars),
    ('scale', StandardScaler(), continuous_vars)
], remainder='passthrough')

# 5. Pipeline avec SMOTE
pipeline = Pipeline([
    ('preprocess', preprocessor),
    ('smote', SMOTE(random_state=42)),
    ('model', RandomForestClassifier(n_estimators=100, random_state=142))
])

# 6. Cross-validation
scores = cross_validate(
    pipeline,
    X, y,
    cv=5,
    scoring=['accuracy', 'precision_macro', 'recall_macro', 'f1_macro'],
    return_train_score=False
)

# 7. R√©sultats
print("‚úÖ Moyenne Accuracy :", round(scores['test_accuracy'].mean(), 3))
print("‚úÖ Moyenne Recall   :", round(scores['test_recall_macro'].mean(), 3))
print("‚úÖ Moyenne Pr√©cision:", round(scores['test_precision_macro'].mean(), 3))
print("‚úÖ Moyenne F1-score :", round(scores['test_f1_macro'].mean(), 3))
print("‚ÑπÔ∏è  √âcart-type Accuracy :", round(np.std(scores['test_accuracy']), 3))

from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import confusion_matrix
import numpy as np
import seaborn as sns

kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=142)

conf_matrices = []

for train_index, test_index in kf.split(X, y):
    X_train_cv, X_test_cv = X.iloc[train_index], X.iloc[test_index]
    y_train_cv, y_test_cv = y.iloc[train_index], y.iloc[test_index]

    pipeline.fit(X_train_cv, y_train_cv)
    y_pred_cv = pipeline.predict(X_test_cv)

    cm = confusion_matrix(y_test_cv, y_pred_cv)
    conf_matrices.append(cm)

# Moyenne des matrices de confusion
avg_conf_matrix = np.mean(conf_matrices, axis=0)

# Affichage
plt.figure(figsize=(6, 5))
sns.heatmap(avg_conf_matrix, annot=True, fmt='.1f', cmap='Blues')
plt.xlabel("Pr√©dit")
plt.ylabel("R√©el")
plt.title("Matrice de confusion moyenne (5-fold CV)")
plt.show()

"""### **E. R√©gression Logistique: Comparaison avec un mod√®le lin√©aire (R√©gression Logistique)**

Pour mieux interpr√©ter les r√©sultats et les variables influentes, nous allons maintenant entra√Æner un mod√®le de R√©gression Logistique.
Ce mod√®le, plus simple et lin√©aire, offre l‚Äôavantage de fournir des coefficients interpr√©tables pour chaque variable.

Cela nous permettra de comparer les performances et de identifier les facteurs de risque les plus associ√©s au cancer.

##### Equilibrage avec Undersampling
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_validate
from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# 1. Chargement des donn√©es
df = pd.read_csv('df_final.csv')
df.replace(-1, pd.NA, inplace=True)
df = df.loc[:, ~df.columns.duplicated()]
target = 'EverHadCancer'

# 2. Variables
binary_vars = ['CutSkipMeals2','DiffPayMedBills','SmokeNow', 'MedConditions_Diabetes','MedConditions_HighBP','MedConditions_HeartCondition',
                 'MedConditions_LungDisease', 'MedConditions_Depression', 'BirthSex', 'FamilyEverHadCancer2']

ordinal_vars = ['GeneralHealth','HealthLimits_Pain', 'Nervous', 'IncomeRanges', 'Education']
string_categorical_vars = [
    'Mexican','PuertoRican','Cuban','OthHisp','Hisp_Cat','White','Black','AmerInd','AsInd',
    'Chinese','Filipino','Japanese','Korean','Vietnamese','OthAsian','OthPacIsl'
]
continuous_vars = [
    'Fruit2', 'Vegetables2', 'TimesStrengthTraining', 'Drink_nb_PerMonth',
    'ChildrenInHH', 'TotalHousehold', 'TimesSunburned', 'BMI', 'Age', 'SleepWeekdayHr'
]

features = binary_vars + ordinal_vars + continuous_vars + string_categorical_vars

# 3. √âquilibrage
df_cancer = df[df[target] == 1]
df_noncancer = df[df[target] == 0].sample(n=len(df_cancer), random_state=142)
df_balanced = pd.concat([df_cancer, df_noncancer]).sample(frac=1, random_state=142).reset_index(drop=True)

# 4. S√©lection et nettoyage
df_model = df_balanced[features + [target]].dropna()
X = df_model[features]
y = df_model[target]

# 9. Split des donn√©es
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=142)

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedKFold

# 5. Pr√©processing
preprocessor = ColumnTransformer([
    ('ord', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), ordinal_vars), # Added handle_unknown and unknown_value
    ('ohe', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), string_categorical_vars),
    ('scale', StandardScaler(), continuous_vars)
], remainder='passthrough')

# 6. Pipeline
pipeline_logreg = Pipeline([
    ('preprocess', preprocessor),
    ('model', LogisticRegression(max_iter=1000, random_state=142))
])

# Validation crois√©e
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=142)

scores_logreg = cross_validate(
    pipeline_logreg,
    X, y,
    cv=cv,
    scoring=['accuracy', 'precision', 'recall']
)

print("‚úÖ Logistic Regression")
print("Accuracy moyenne :", round(scores_logreg['test_accuracy'].mean(), 3))
print("Recall moyen     :", round(scores_logreg['test_recall'].mean(), 3))
print("Pr√©cision moyenne :", round(scores_logreg['test_precision'].mean(), 3))

"""##### Equilibrage avec SMOTE"""

from sklearn.model_selection import cross_validate
import pandas as pd
import numpy as np
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler

# 1. Chargement
df = pd.read_csv('df_final.csv')
df.replace(-1, pd.NA, inplace=True)
df = df.loc[:, ~df.columns.duplicated()]
target = 'EverHadCancer'

# 2. Variables
binary_vars = ['CutSkipMeals2','DiffPayMedBills','SmokeNow', 'MedConditions_Diabetes','MedConditions_HighBP','MedConditions_HeartCondition',
                 'MedConditions_LungDisease', 'MedConditions_Depression', 'BirthSex', 'FamilyEverHadCancer2']

ordinal_vars = ['GeneralHealth','HealthLimits_Pain', 'Nervous', 'IncomeRanges', 'Education']
string_categorical_vars = [
    'Mexican','PuertoRican','Cuban','OthHisp','Hisp_Cat','White','Black','AmerInd','AsInd',
    'Chinese','Filipino','Japanese','Korean','Vietnamese','OthAsian','OthPacIsl'
]
continuous_vars = [
    'Fruit2', 'Vegetables2', 'TimesStrengthTraining', 'Drink_nb_PerMonth',
    'ChildrenInHH', 'TotalHousehold', 'TimesSunburned', 'BMI', 'Age', 'SleepWeekdayHr'
]
features = binary_vars + ordinal_vars + continuous_vars + string_categorical_vars

# 3. Donn√©es
df_model = df[features + [target]].dropna()
X = df_model[features]
y = df_model[target]

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedKFold

# 5. Pr√©processing
preprocessor = ColumnTransformer([
    ('ord', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1), ordinal_vars), # Added handle_unknown and unknown_value
    ('ohe', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), string_categorical_vars),
    ('scale', StandardScaler(), continuous_vars)
], remainder='passthrough')

# 6. Pipeline
pipeline_logreg = Pipeline([
    ('preprocess', preprocessor),
    ('smote', SMOTE(random_state=42)),  # ‚úÖ √©tape SMOTE ajout√©e ici
    ('model', LogisticRegression(max_iter=1000, random_state=142))
])


# Validation crois√©e
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=142)

scores_logreg = cross_validate(
    pipeline_logreg,
    X, y,
    cv=cv,
    scoring=['accuracy', 'precision', 'recall']
)

print("‚úÖ Logistic Regression")
print("Accuracy moyenne :", round(scores_logreg['test_accuracy'].mean(), 3))
print("Recall moyen     :", round(scores_logreg['test_recall'].mean(), 3))
print("Pr√©cision moyenne :", round(scores_logreg['test_precision'].mean(), 3))

"""## **Conclusion**

Dans ce projet, nous avons construit et compar√© plusieurs mod√®les pr√©dictifs afin d‚Äôidentifier les facteurs associ√©s √† la survenue d‚Äôun cancer.

Nous avons suivi une d√©marche rigoureuse en plusieurs √©tapes :

Un nettoyage des donn√©es (valeurs manquantes, variables dupliqu√©es),
Une pr√©paration des variables via encodage et normalisation,
Un √©quilibrage du jeu de donn√©es avec undersampling et SMOTE,

Puis des exp√©rimentations avec plusieurs mod√®les :


*   Random Forest,
*   R√©gression logistique,
*   Et une validation crois√©e pour stabiliser les r√©sultats.


Les performances des mod√®les (accuracy, recall, pr√©cision) ont montr√© que :


*   Le **Random Forest** fournit une bonne performance globale avec des variables
importantes comme l‚Äô√¢ge, le sommeil, l'alimentation ou les difficult√©s financi√®res.
*   La **R√©gression Logistique** apporte une meilleure **interpr√©tabilit√©**, en permettant d‚Äôobserver clairement les effets **positifs ou n√©gatifs** des variables.
*   L'ajout du **SMOTE** a permis d'am√©liorer la sensibilit√© (recall) du mod√®le sans trop d√©grader la pr√©cision.

Ce travail ouvre la voie √† des applications utiles en pr√©vention, en identifiant les profils √† risque, mais n√©cessite d‚Äô√™tre valid√© sur d‚Äôautres √©chantillons pour une g√©n√©ralisation.

Pour une application en production, nous recommanderions donc d‚Äôopter pour Random Forest + undersampling + validation crois√©e. Cela permet d‚Äôavoir un mod√®le plus juste, capable de d√©tecter les cas positifs tout en limitant les fausses alertes.

### VII. Mod√®le Random Forest avec undersampling et validation crois√©e
"""

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OrdinalEncoder, StandardScaler, OneHotEncoder
from sklearn.metrics import make_scorer, accuracy_score, recall_score, precision_score, f1_score
from sklearn.utils import resample

# 1. Chargement des donn√©es
df = pd.read_csv('df_final.csv')

# 2. Nettoyage : remplacer les -1 par NaN
df.replace(-1, pd.NA, inplace=True)
df = df.loc[:, ~df.columns.duplicated()]

# 3. Filtrage des colonnes utiles
binary_vars = ['CutSkipMeals2','DiffPayMedBills','SmokeNow', 'MedConditions_Diabetes','MedConditions_HighBP','MedConditions_HeartCondition',
                 'MedConditions_LungDisease', 'MedConditions_Depression', 'BirthSex', 'FamilyEverHadCancer2']

ordinal_vars = ['GeneralHealth','HealthLimits_Pain', 'Nervous', 'IncomeRanges', 'Education']
string_vars = [
    'Mexican','PuertoRican','Cuban','OthHisp','Hisp_Cat','White','Black','AmerInd','AsInd',
    'Chinese','Filipino','Japanese','Korean','Vietnamese','OthAsian','OthPacIsl'
]
continuous_vars = [
    'Fruit2', 'Vegetables2', 'TimesStrengthTraining', 'Drink_nb_PerMonth',
    'ChildrenInHH', 'TotalHousehold', 'TimesSunburned', 'BMI', 'Age', 'SleepWeekdayHr'
]
target = 'EverHadCancer'

features = binary_vars + ordinal_vars + string_vars + continuous_vars
df = df[features + [target]].dropna()

# 3. Undersampling
df_minority = df[df[target] == 1]# personne atteint d'un Cancer
df_majority = df[df[target] == 0].sample(n=len(df_cancer), random_state=142)# personne non atteint d'un Cancer
df_majority_down = resample(df_majority, replace=False, n_samples=len(df_minority), random_state=142)
df_balanced = pd.concat([df_majority_down, df_minority]).sample(frac=1, random_state=142)

X = df_balanced[features]
y = df_balanced[target]

# 4. Pr√©processing
preprocessor = ColumnTransformer(transformers=[
    ('ord', OrdinalEncoder(), ordinal_vars),
    ('ohe', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), string_vars),
    ('scale', StandardScaler(), continuous_vars)
], remainder='passthrough')

# 5. Pipeline complet
pipeline = Pipeline([
    ('preprocess', preprocessor),
    ('model', RandomForestClassifier(n_estimators=100, random_state=42))
])

# 6. Cross-validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scoring = {
    'accuracy': make_scorer(accuracy_score),
    'recall': make_scorer(recall_score),
    'precision': make_scorer(precision_score),
    'f1': make_scorer(f1_score)
}

# 7. R√©sultats
results = {}
for name, scorer in scoring.items():
    scores = cross_val_score(pipeline, X, y, cv=cv, scoring=scorer)
    results[name] = (scores.mean(), scores.std())

# 8. Affichage
print("‚úÖ R√©sultats (Random Forest + undersampling + CV) :")
for metric, (mean, std) in results.items():
    print(f"{metric.capitalize():<10} : {mean:.3f} (¬±{std:.3f})")

"""#####Conclusion :

Les r√©sultats montrent que le mod√®le Random Forest avec undersampling et validation crois√©e pr√©sente un bon compromis entre rappel √©lev√© (bonne d√©tection des cas positifs), pr√©cision acceptable (peu de fausses alertes) et stabilit√© des r√©sultats. Il appara√Æt ainsi comme le mod√®le le plus fiable et √©quilibr√© pour une √©ventuelle mise en production.

Nous recommandons ce mod√®le comme solution par d√©faut pour un syst√®me automatis√© de d√©tection de risque de cancer, en raison de sa robustesse, de sa capacit√© √† g√©rer les donn√©es d√©s√©quilibr√©es et de ses performances globales solides.

"""